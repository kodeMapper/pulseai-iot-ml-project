import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score
from imblearn.over_sampling import SMOTE
import xgboost as xgb
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import warnings
warnings.filterwarnings('ignore')

# Load dataset
df = pd.read_csv('maternal_health_risk.csv')
X = df.drop('RiskLevel', axis=1)
y = df['RiskLevel']

# Encode labels
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

print("=" * 80)
print("MODEL IMPROVEMENT ANALYSIS - Finding Best Configuration")
print("=" * 80)

print(f"\nDataset Info:")
print(f"  Total samples: {len(df)}")
print(f"  Training samples: {len(X_train)} (original) -> {len(X_train_resampled)} (after SMOTE)")
print(f"  Test samples: {len(X_test)}")
print(f"  Classes: {le.classes_}")

# Store results
results = []

# ============================================================================
# TEST 1: Current XGBoost (default parameters)
# ============================================================================
print("\n" + "=" * 80)
print("TEST 1: XGBoost with Default Parameters (CURRENT MODEL)")
print("=" * 80)

xgb_default = xgb.XGBClassifier(
    objective='multi:softmax',
    num_class=3,
    random_state=42,
    eval_metric='mlogloss'
)
xgb_default.fit(X_train_resampled, y_train_resampled)
y_pred = xgb_default.predict(X_test_scaled)

accuracy = accuracy_score(y_test, y_pred)
high_risk_recall = recall_score(y_test, y_pred, labels=[0], average='macro')

print(f"\nAccuracy: {accuracy*100:.2f}%")
print(f"High-Risk Recall: {high_risk_recall*100:.2f}%")
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=le.classes_))

cm = confusion_matrix(y_test, y_pred)
high_risk_idx = 0
false_negatives = cm[high_risk_idx].sum() - cm[high_risk_idx][high_risk_idx]
print(f"\n‚ùå FALSE NEGATIVES (High-Risk Missed): {false_negatives}/{cm[high_risk_idx].sum()}")

results.append({
    'Model': 'XGBoost (Default)',
    'Accuracy': accuracy * 100,
    'High-Risk Recall': high_risk_recall * 100,
    'False Negatives': false_negatives
})

# ============================================================================
# TEST 2: XGBoost with Optimized Parameters for Medical Use
# ============================================================================
print("\n" + "=" * 80)
print("TEST 2: XGBoost with Medical-Focused Parameters")
print("=" * 80)
print("Strategy: Prioritize recall over precision (reduce false negatives)")

xgb_medical = xgb.XGBClassifier(
    objective='multi:softmax',
    num_class=3,
    max_depth=6,
    learning_rate=0.05,
    n_estimators=200,
    min_child_weight=1,
    subsample=0.8,
    colsample_bytree=0.8,
    scale_pos_weight=2,  # Give more weight to minority class
    random_state=42,
    eval_metric='mlogloss'
)
xgb_medical.fit(X_train_resampled, y_train_resampled)
y_pred = xgb_medical.predict(X_test_scaled)

accuracy = accuracy_score(y_test, y_pred)
high_risk_recall = recall_score(y_test, y_pred, labels=[0], average='macro')

print(f"\nAccuracy: {accuracy*100:.2f}%")
print(f"High-Risk Recall: {high_risk_recall*100:.2f}%")
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=le.classes_))

cm = confusion_matrix(y_test, y_pred)
false_negatives = cm[high_risk_idx].sum() - cm[high_risk_idx][high_risk_idx]
print(f"\n‚ùå FALSE NEGATIVES (High-Risk Missed): {false_negatives}/{cm[high_risk_idx].sum()}")

results.append({
    'Model': 'XGBoost (Medical-Tuned)',
    'Accuracy': accuracy * 100,
    'High-Risk Recall': high_risk_recall * 100,
    'False Negatives': false_negatives
})

# ============================================================================
# TEST 3: Random Forest
# ============================================================================
print("\n" + "=" * 80)
print("TEST 3: Random Forest Classifier")
print("=" * 80)

rf_model = RandomForestClassifier(
    n_estimators=200,
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=2,
    class_weight='balanced',
    random_state=42
)
rf_model.fit(X_train_resampled, y_train_resampled)
y_pred = rf_model.predict(X_test_scaled)

accuracy = accuracy_score(y_test, y_pred)
high_risk_recall = recall_score(y_test, y_pred, labels=[0], average='macro')

print(f"\nAccuracy: {accuracy*100:.2f}%")
print(f"High-Risk Recall: {high_risk_recall*100:.2f}%")
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=le.classes_))

cm = confusion_matrix(y_test, y_pred)
false_negatives = cm[high_risk_idx].sum() - cm[high_risk_idx][high_risk_idx]
print(f"\n‚ùå FALSE NEGATIVES (High-Risk Missed): {false_negatives}/{cm[high_risk_idx].sum()}")

results.append({
    'Model': 'Random Forest',
    'Accuracy': accuracy * 100,
    'High-Risk Recall': high_risk_recall * 100,
    'False Negatives': false_negatives
})

# ============================================================================
# TEST 4: Gradient Boosting
# ============================================================================
print("\n" + "=" * 80)
print("TEST 4: Gradient Boosting Classifier")
print("=" * 80)

gb_model = GradientBoostingClassifier(
    n_estimators=200,
    learning_rate=0.05,
    max_depth=5,
    min_samples_split=5,
    min_samples_leaf=2,
    subsample=0.8,
    random_state=42
)
gb_model.fit(X_train_resampled, y_train_resampled)
y_pred = gb_model.predict(X_test_scaled)

accuracy = accuracy_score(y_test, y_pred)
high_risk_recall = recall_score(y_test, y_pred, labels=[0], average='macro')

print(f"\nAccuracy: {accuracy*100:.2f}%")
print(f"High-Risk Recall: {high_risk_recall*100:.2f}%")
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=le.classes_))

cm = confusion_matrix(y_test, y_pred)
false_negatives = cm[high_risk_idx].sum() - cm[high_risk_idx][high_risk_idx]
print(f"\n‚ùå FALSE NEGATIVES (High-Risk Missed): {false_negatives}/{cm[high_risk_idx].sum()}")

results.append({
    'Model': 'Gradient Boosting',
    'Accuracy': accuracy * 100,
    'High-Risk Recall': high_risk_recall * 100,
    'False Negatives': false_negatives
})

# ============================================================================
# SUMMARY & RECOMMENDATION
# ============================================================================
print("\n" + "=" * 80)
print("COMPARISON SUMMARY")
print("=" * 80)

results_df = pd.DataFrame(results)
print(results_df.to_string(index=False))

print("\n" + "=" * 80)
print("RECOMMENDATION")
print("=" * 80)

# Find best model (prioritize low false negatives, then high accuracy)
best_model = results_df.loc[results_df['False Negatives'].idxmin()]

print(f"\n‚úÖ BEST MODEL: {best_model['Model']}")
print(f"   - Accuracy: {best_model['Accuracy']:.2f}%")
print(f"   - High-Risk Recall: {best_model['High-Risk Recall']:.2f}%")
print(f"   - False Negatives: {int(best_model['False Negatives'])}")

print(f"\nüí° ANALYSIS:")
print(f"   Current model (XGBoost Default) has {results[0]['False Negatives']:.0f} false negatives")

if best_model['False Negatives'] < results[0]['False Negatives']:
    improvement = results[0]['False Negatives'] - best_model['False Negatives']
    print(f"   ‚ú® We can reduce false negatives by {improvement:.0f} cases!")
    print(f"   ‚ú® Accuracy change: {results[0]['Accuracy']:.2f}% ‚Üí {best_model['Accuracy']:.2f}%")
    print(f"\n   RECOMMENDATION: Switch to {best_model['Model']}")
elif best_model['Accuracy'] > results[0]['Accuracy'] + 1:
    print(f"   ‚ú® We can improve accuracy by {best_model['Accuracy']-results[0]['Accuracy']:.2f}%")
    print(f"   ‚ú® While maintaining same false negative rate")
    print(f"\n   RECOMMENDATION: Consider switching to {best_model['Model']}")
else:
    print(f"   ‚úÖ Current model is already optimal!")
    print(f"   ‚úÖ 92%+ accuracy with minimal false negatives")
    print(f"\n   RECOMMENDATION: Keep current XGBoost model")

print("\n" + "=" * 80)
